<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scala on YPG Data</title>
    <link>https://ypg-data.github.io/tags/scala/</link>
    <description>Recent content in Scala on YPG Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Jul 2016 20:47:14 -0400</lastBuildDate>
    <atom:link href="https://ypg-data.github.io/tags/scala/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Simple Spark ml pipeline</title>
      <link>https://ypg-data.github.io/post/2016/07/simple-spark-ml-pipeline/</link>
      <pubDate>Sun, 10 Jul 2016 20:47:14 -0400</pubDate>
      
      <guid>https://ypg-data.github.io/post/2016/07/simple-spark-ml-pipeline/</guid>
      <description>

&lt;p&gt;Mediative recently hosted a &lt;a href=&#34;http://www.meetup.com/Montreal-Apache-Spark-Meetup/events/231285569/&#34;&gt;Apache Spark Montreal Meetup&lt;/a&gt;&amp;rsquo;s project night where some of us decided to create a simple ML pipeline.
To spare the installation of Spark, we used the &lt;a href=&#34;https://databricks.com/try-databricks&#34;&gt;Databricks community edition&lt;/a&gt;.
Since the goal was to see if we could make it work,
we wanted to use data that we knew was correlated.
But to make the project a little more fun,
we decided to explore something else than the usual &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_set#Classic_data_sets&#34;&gt;data sets&lt;/a&gt;
so we went for the Dow Jones and Nasdaq.
In the wealth of all &lt;code&gt;R&lt;/code&gt; packages, one can find the &lt;a href=&#34;https://cran.r-project.org/web/packages/quantmod/quantmod.pdf&#34;&gt;quantmod package&lt;/a&gt; to access financial data.&lt;/p&gt;

&lt;h2 id=&#34;getting-data&#34;&gt;Getting data&lt;/h2&gt;

&lt;p&gt;The notebook was created for &lt;code&gt;Scala&lt;/code&gt; language, so we used the &lt;code&gt;%r&lt;/code&gt; magic to install and use the &lt;code&gt;R&lt;/code&gt; package to access the data.  And while we were at it, we merge both data sets right away:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;%r
install.packages(&amp;quot;quantmod&amp;quot;)
library(&amp;quot;quantmod&amp;quot;)

## NASDAQ
nsd&amp;lt;-as.data.frame(getSymbols(Symbols = &amp;quot;^NDX&amp;quot;,
    src = &amp;quot;yahoo&amp;quot;, from = &amp;quot;2015-01-01&amp;quot;,to = &amp;quot;2016-01-01&amp;quot;, env = NULL))
## Dow Jones
dji&amp;lt;-as.data.frame(getSymbols(Symbols = &amp;quot;^DJI&amp;quot;,
    src = &amp;quot;yahoo&amp;quot;, from = &amp;quot;2015-01-01&amp;quot;,to = &amp;quot;2016-01-01&amp;quot;, env = NULL))

## Adding the date as a column (in the above they are index and are lost when a table is created)
nsd$date&amp;lt;-rownames(nsd)
dji$date&amp;lt;-rownames(dji)

## Merge the tables together on date
mrgIndx&amp;lt;-merge(nsd, dji)

dfIndx &amp;lt;- createDataFrame(sqlContext, mrgIndx)
registerTempTable(dfIndx, &amp;quot;testIndx&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last line register the dataframe as a (temporary) table to make it available outside of the &lt;code&gt;R&lt;/code&gt; scope.&lt;/p&gt;

&lt;p&gt;The following (default) scala cell will create a dataframe back from this table.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = sqlContext.sql(&amp;quot;SELECT * FROM testIndx&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;looking-at-the-data&#34;&gt;Looking at the data&lt;/h2&gt;

&lt;p&gt;Of all the fields, we will only consider the &lt;code&gt;date&lt;/code&gt; and the adjusted Nasdaq,&lt;code&gt;NDX_Adjusted&lt;/code&gt;, and Dow Jones, &lt;code&gt;DJI_Adjusted&lt;/code&gt; values.  Why adjusted? No reason, so why not!  Let see if they are correlated and we can have hope to predict one with the other:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.functions.lit
display(df.withColumn(&amp;quot;NDXTimes5&amp;quot;, $&amp;quot;NDX_Adjusted&amp;quot;.cast(DoubleType).multiply(lit(5))))
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://ypg-data.github.io/images/simple-ml-pipeline/djiNndx5.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;No fancy statistical tools are needed to see that these two curves are correlated. The Nasdaq value has been scaled up by five (using the imported &lt;code&gt;lit&lt;/code&gt; function) to make the comparison more obvious, but this scaling will not be used in the training.  Hopefully, even a basic model can take care of that.&lt;/p&gt;

&lt;h2 id=&#34;preparing-the-data-and-the-model&#34;&gt;Preparing the data and the model&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s keep only the fields that we will need&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val data = df.withColumn(&amp;quot;NDX&amp;quot;, $&amp;quot;NDX_Adjusted&amp;quot;)
.withColumn(&amp;quot;DJI&amp;quot;, $&amp;quot;DJI_Adjusted&amp;quot;)
.select(&amp;quot;NDX&amp;quot;, &amp;quot;DJI&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And let&amp;rsquo;s keep a random test subsample for testing purpose, the rest will be use for training the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val Array(training, test) = data.randomSplit(Array(0.75, 0.25), seed = 12345)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the &lt;code&gt;VectorAssembler&lt;/code&gt; to create the feature vector used by the model.  We want to predict the Dow Jones with the Nasdaq (&lt;code&gt;NDX&lt;/code&gt;), so the latter will be our feature, which we will wisely call &lt;code&gt;features&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.ml.feature.VectorAssembler
val assembler = new VectorAssembler()
  .setInputCols(Array(&amp;quot;NDX&amp;quot;))
  .setOutputCol(&amp;quot;features&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will also need a model to learn with, for such a simple task, let&amp;rsquo;s use a simple linear regression where we define the Dow Jones (&lt;code&gt;DJI&lt;/code&gt;) as the target we want to learn on (that is called &lt;code&gt;label&lt;/code&gt; in &lt;code&gt;ml&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression()
  .setLabelCol(&amp;quot;DJI&amp;quot;)
  .setFeaturesCol(&amp;quot;features&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;set-up-the-pipeline&#34;&gt;Set up the pipeline&lt;/h2&gt;

&lt;p&gt;Now that we have all the elements, we can easily assemble them with the &lt;code&gt;pipeline&lt;/code&gt; functionality.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.ml.Pipeline
val steps: Array[org.apache.spark.ml.PipelineStage] = Array(assembler, lr)
val pipeline = new Pipeline().setStages(steps)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fitting-the-model&#34;&gt;Fitting the model&lt;/h2&gt;

&lt;p&gt;Preparing data and training is done with a single call of the pipeline&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val myModel = pipeline.fit(training)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now see how well the model works by comparing its prediction with the actual Dow Jones values&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;display(myModel.transform(test).select(&amp;quot;prediction&amp;quot;, &amp;quot;DJI&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://ypg-data.github.io/images/simple-ml-pipeline/predNactual.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;The model obviously managed to learn correlations between the Dow jones and the Nasdaq.  Nothing to impress your broker, but that is a basis on which building better prediction.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>